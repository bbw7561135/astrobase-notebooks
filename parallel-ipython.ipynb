{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Using `ipyparallel` clusters\n",
    "\n",
    "This is a terse intro to using `ipyparallel` clusters to do HAT stuff on the `shrike` cluster. The following packages are installed in the virtualenv for all machines in the cluster.\n",
    "\n",
    "- ipyparallel\n",
    "- joblib\n",
    "- dask\n",
    "- numpy\n",
    "- scipy\n",
    "- matplotlib\n",
    "- astropy\n",
    "- astroquery\n",
    "- astrobase\n",
    "\n",
    "plus all of their dependencies.\n",
    "\n",
    "All machines have identical home directories and identical virtualenvs.  Alternatively (something I should do actually), we could install the venv in the cluster-wide `/nfs/shrike/ar0` directory and run it from there.\n",
    "\n",
    "The cluster is live as of 2016-12-08, and consists of the following machines:\n",
    "\n",
    "- `shrike`: head node with Xeon-D 1520 4-core (8-thread) CPU at 2.0 Ghz, 64 GB ECC DDR4 RAM, 2 x 300 GB DC3500 SSDs (one for / and one for /nfs/shrike/ar0)\n",
    "- `cluster-i7-one`: worker node with i7-6700 4-core (8-thread) at 3.4 Ghz, 16 GB DDR4 RAM, 1 x 120 GB SSD for /\n",
    "- `cluster-i7-two`: worker node with i7-6700 4-core (8-thread) at 3.4 Ghz, 16 GB DDR4 RAM, 1 x 120 GB SSD for /\n",
    "- `cluster-i5-one`: worker node with i5-6500 4-core (4-thread) at 3.2 Ghz, 16 GB DDR4 RAM, 1 x 120 GB SSD for /\n",
    "- `cluster-i5-two`: worker node with i5-6500 4-core (4-thread) at 3.2 Ghz, 16 GB DDR4 RAM, 1 x 120 GB SSD for /\n",
    "\n",
    "I plan to add three more cluster members to bring this up to an even eight nodes. One of these will likely be some sort of GPU machine, using a GTX1060 6GB card, so we can figure out GPU period-finding.\n",
    "\n",
    "## Mapping functions across the whole cluster\n",
    "\n",
    "The following explains how to run map operations across the whole cluster. This will dispatch functions to all nodes, run them, and return the results. This is akin to the usual `multiprocessing.map` we use all the time. All of the following stuff is done on the head node, in an ipython terminal console or in a Jupyter notebook.\n",
    "\n",
    "### connect to the cluster\n",
    "\n",
    "The cluster head node is `shrike`. SSH in and activate the virtualenv to get started.\n",
    "\n",
    "```bash\n",
    "[user@shrike]$ source venv/bin/activate\n",
    "```\n",
    "\n",
    "Then start ipython on the terminal using: \n",
    "\n",
    "```bash\n",
    "(venv) [user@shrike]$ ipython\n",
    "```\n",
    "\n",
    "Or use the Jupyter notebook. For this, you'll have to first set up an SSH tunnel between your machine and `shrike`:\n",
    "\n",
    "```bash\n",
    "# start the tunnel\n",
    "[user@local]$ ssh -L localhost:8888:localhost:8888 user@shrike\n",
    "\n",
    "# start the virtualenv\n",
    "[user@shrike]$ source venv/bin/activate\n",
    "(venv) [user@shrike]$ ipython notebook --no-browser --port=8888\n",
    "```\n",
    "\n",
    "Then on your local machine, browse to http://localhost:8888.\n",
    "\n",
    "### set up the cluster view"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cluster nodes visible: [0, 1, 2, 3]\n"
     ]
    }
   ],
   "source": [
    "from ipyparallel import Client\n",
    "rc = Client()\n",
    "print ('cluster nodes visible: %s' % rc.ids)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# DirectView object to interact with the cluster\n",
    "dview = rc[:]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### an example of running period finding across the whole cluster"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import os.path"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "u'/home/wbhatti'"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pwd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/nfs/shrike/ar0/work/wbhatti/scratch\n"
     ]
    }
   ],
   "source": [
    "cd /nfs/shrike/ar0/work/wbhatti/scratch/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# we're sitting in the /nfs/shrike/ar0/work/wbhatti/scratch directory\n",
    "# this is an NFS volume shared across the whole cluster\n",
    "lclist = !ls *.sqlite.gz"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "lclist = [os.path.abspath(x) for x in lclist]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['/nfs/shrike/ar0/work/wbhatti/scratch/HAT-432-0007388-V0-DR0-hatlc.sqlite.gz',\n",
       " '/nfs/shrike/ar0/work/wbhatti/scratch/HAT-553-0087416-V0-DR0-hatlc.sqlite.gz',\n",
       " '/nfs/shrike/ar0/work/wbhatti/scratch/HAT-772-0212353-V0-DR0-hatlc.sqlite.gz',\n",
       " '/nfs/shrike/ar0/work/wbhatti/scratch/HAT-772-0215246-V0-DR0-hatlc.sqlite.gz',\n",
       " '/nfs/shrike/ar0/work/wbhatti/scratch/HAT-772-0215592-V0-DR0-hatlc.sqlite.gz',\n",
       " '/nfs/shrike/ar0/work/wbhatti/scratch/HAT-772-0219865-V0-DR0-hatlc.sqlite.gz',\n",
       " '/nfs/shrike/ar0/work/wbhatti/scratch/HAT-772-0302504-V0-DR0-hatlc.sqlite.gz',\n",
       " '/nfs/shrike/ar0/work/wbhatti/scratch/HAT-772-0554686-V0-DR0-hatlc.sqlite.gz',\n",
       " '/nfs/shrike/ar0/work/wbhatti/scratch/HAT-772-0562164-V0-DR0-hatlc.sqlite.gz',\n",
       " '/nfs/shrike/ar0/work/wbhatti/scratch/HAT-777-0058978-V0-DR0-hatlc.sqlite.gz']"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "lclist"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "importing periodbase from astrobase on engine(s)\n",
      "importing hatlc from astrobase on engine(s)\n"
     ]
    }
   ],
   "source": [
    "# first: make all nodes import the things we need\n",
    "with dview.sync_imports():\n",
    "    from astrobase import periodbase\n",
    "    from astrobase import hatlc\n",
    "    import gzip\n",
    "    import cPickle\n",
    "    \n",
    "# note that importing things like 'import os.path' will fail for some weird reason I haven't figured out yet\n",
    "# best to do 'from os import path' instead\n",
    "\n",
    "# also note that importing things like 'import cPickle as pickle' will fail\n",
    "# in this case, just import cPickle directly\n",
    "\n",
    "# I think these only fail when broadcasting imports across the cluster with the sync_imports statement\n",
    "# doing these internally in the imported modules appears to work OK\n",
    "\n",
    "# finally, you can just send stuff to all nodes and make them available that way\n",
    "# >>> dview.push(an_object) # where an_object can be an imported module"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# this function will be mapped across the whole cluster\n",
    "def make_lsp(lcf):\n",
    "    lcd, msg = hatlc.read_and_filter_sqlitecurve(lcf)\n",
    "    normlcd = hatlc.normalize_lcdict(lcd)\n",
    "    times, mags, errs = lcd['rjd'], lcd['aep_000'], lcd['aie_000']\n",
    "    lsp = periodbase.aov_periodfind(times,mags,errs)\n",
    "    outpkl = lcf.replace('hatlc.sqlite.gz','aov-lsp.pkl.gz') # this will write to the NFS shared storage\n",
    "    with gzip.open(outpkl,'wb') as outfd:\n",
    "        cPickle.dump(lsp,outfd,protocol=cPickle.HIGHEST_PROTOCOL)\n",
    "    return lcf, lsp # probably slow when returning large objects back to the head node here\n",
    "                    # better to return the output filename as a string instead"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# this function call maps the function above to all worker nodes\n",
    "# and blocks until we get results back\n",
    "results = dview.map_sync(make_lsp, lclist)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# make sure we got everything\n",
    "len(results) == len(lclist)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/nfs/shrike/ar0/work/wbhatti/scratch/HAT-432-0007388-V0-DR0-hatlc.sqlite.gz: 7.30451\n",
      "/nfs/shrike/ar0/work/wbhatti/scratch/HAT-553-0087416-V0-DR0-hatlc.sqlite.gz: 0.16940\n",
      "/nfs/shrike/ar0/work/wbhatti/scratch/HAT-772-0212353-V0-DR0-hatlc.sqlite.gz: 345.35128\n",
      "/nfs/shrike/ar0/work/wbhatti/scratch/HAT-772-0215246-V0-DR0-hatlc.sqlite.gz: 518.02692\n",
      "/nfs/shrike/ar0/work/wbhatti/scratch/HAT-772-0215592-V0-DR0-hatlc.sqlite.gz: 70.24094\n",
      "/nfs/shrike/ar0/work/wbhatti/scratch/HAT-772-0219865-V0-DR0-hatlc.sqlite.gz: 8.23900\n",
      "/nfs/shrike/ar0/work/wbhatti/scratch/HAT-772-0302504-V0-DR0-hatlc.sqlite.gz: 2.07004\n",
      "/nfs/shrike/ar0/work/wbhatti/scratch/HAT-772-0554686-V0-DR0-hatlc.sqlite.gz: 3.08579\n",
      "/nfs/shrike/ar0/work/wbhatti/scratch/HAT-772-0562164-V0-DR0-hatlc.sqlite.gz: 6.06767\n",
      "/nfs/shrike/ar0/work/wbhatti/scratch/HAT-777-0058978-V0-DR0-hatlc.sqlite.gz: 0.64831\n"
     ]
    }
   ],
   "source": [
    "for lcf, lsp in results:\n",
    "    print('%s: %.5f' % (lcf, lsp['bestperiod']))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.12"
  },
  "latex_envs": {
   "bibliofile": "biblio.bib",
   "cite_by": "apalike",
   "current_citInitial": 1,
   "eqLabelWithNumbers": true,
   "eqNumInitial": 0
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
